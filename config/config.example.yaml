# Code Quality Intelligence Configuration Example
# Copy this file to config.yaml and customize as needed

# AI Agent Configuration
agent:
  google_api_key: ${GOOGLE_API_KEY}  # Set via environment variable
  model_name: gemini-1.5-pro
  temperature: 0.1
  max_tokens: 8192
  timeout: 60
  enable_caching: true
  cache_dir: ~/.cqi/cache
  
  # Local LLM Configuration (Ollama)
  use_local: false  # Set to true to use Ollama instead of Google Gemini
  ollama_model: llama3.2  # Options: llama3.2, mistral, codellama, etc.

# Analyzer Configuration  
analyzer:
  enable_parallel: true
  max_workers: 4
  severity_threshold: low
  ignore_patterns:
    - "*.min.js"
    - "*.generated.*"
    - "node_modules/**"
    - "venv/**"
    - ".git/**"
  custom_rules:
    # Add custom rules here

# Project Settings
project_root: .
output_dir: ./cqi_reports
verbose: false

# Language-specific Settings
python:
  max_line_length: 88
  complexity_threshold: 10

javascript:
  prefer_const: true
  semicolons: true

# Security Settings
security:
  check_secrets: true
  scan_dependencies: true
